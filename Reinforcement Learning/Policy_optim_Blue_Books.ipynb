{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "青本6.3.2の式を実装しながら考えていきましょう"
      ],
      "metadata": {
        "id": "ig4dzrbyE5RC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "$$\n",
        "f_{\\infty}(\\boldsymbol{\\theta})=\\sum_{s \\in \\mathcal{S}} \\sum_{a \\in \\mathcal{A}} p_{\\infty}^{\\pi_{\\boldsymbol{\\theta}}}(s) \\pi_{\\boldsymbol{\\theta}}(a \\mid s) g(s, a) ...(3)\n",
        "$$"
      ],
      "metadata": {
        "id": "J5sd2CmsF2I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 状態空間と行動空間の定義\n",
        "S = [1, 2, 3]  # 状態のリスト\n",
        "A = [0, 1]     # 行動のリスト\n",
        "\n",
        "# サンプル状態価値関数 g(s, a) の定義 (ダミーデータ)\n",
        "def g(s, a):\n",
        "    return torch.rand(1)\n",
        "\n",
        "# サンプル方策 pi_theta(a | s) の定義 (ダミーデータ)\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, num_actions, num_states):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Linear(1, num_actions)\n",
        "\n",
        "    def forward(self, state):\n",
        "        action_scores = self.fc(state)\n",
        "        action_prob = torch.softmax(action_scores, dim=-1)\n",
        "        return action_prob\n",
        "\n",
        "# ポリシーネットワークの初期化\n",
        "num_actions = len(A)\n",
        "num_states = len(S)\n",
        "policy_net = PolicyNetwork(num_actions, num_states)\n",
        "\n",
        "# 状態価値関数 g(s, a) と方策 pi_theta(a | s) を使用して f_infinity(theta) を計算\n",
        "def f_infinity(theta):\n",
        "    total_value = 0\n",
        "    for s in S:\n",
        "        for a in A:\n",
        "            state_tensor = torch.tensor([s], dtype=torch.float32)\n",
        "            action_prob = policy_net(state_tensor)\n",
        "            value = g(s, a)\n",
        "            total_value += value * action_prob[a]  # .item()を使用して値を取得\n",
        "    return total_value\n",
        "\n",
        "# thetaの初期値\n",
        "theta = torch.rand(1, num_states)\n",
        "\n",
        "# f_infinity(theta)の計算\n",
        "result = f_infinity(theta)\n",
        "\n",
        "print(\"f_infinity(theta) =\", result)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "751kKj8cGB70",
        "outputId": "a527b080-c53e-45dd-fb4b-d638ea0f062d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f_infinity(theta) = tensor([1.7270], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bYPswOZtSkSM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}