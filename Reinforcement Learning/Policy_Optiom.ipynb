{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 方策の関数近似\n",
        "REINFORCE \\\\\n",
        "A3C \\\\\n",
        "A2C \\\\\n",
        "DDPG \\\\\n",
        "それぞれ実装していこうかと思います。"
      ],
      "metadata": {
        "id": "I8hSawqY4yRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "\n",
        "# 強化学習環境の設定\n",
        "env = gym.make('CartPole-v1')\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CArN09gsBD5m",
        "outputId": "ecc96520-f8e2-45a8-c0e6-298e93cd5aff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return torch.softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "OwjG8MnqBGi_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy_net = PolicyNetwork()\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "id": "bni2FnATBI0P"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_policy(episode_states, episode_actions, episode_rewards):\n",
        "    discounted_rewards = []\n",
        "    running_add = 0\n",
        "    for r in episode_rewards[::-1]:\n",
        "        running_add = running_add * 0.99 + r\n",
        "        discounted_rewards.append(running_add)\n",
        "    discounted_rewards = torch.tensor(discounted_rewards[::-1])\n",
        "\n",
        "    # NumPyの配列をPyTorchのテンソルに変換\n",
        "    episode_states = torch.tensor(episode_states, dtype=torch.float32)\n",
        "    episode_actions = torch.tensor(episode_actions, dtype=torch.int64)\n",
        "\n",
        "    policy_probs = policy_net(episode_states)\n",
        "    chosen_action_probs = torch.gather(policy_probs, 1, episode_actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "    loss = -torch.sum(torch.log(chosen_action_probs) * discounted_rewards)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "zde2mlygBKoa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 1000\n",
        "episode_states, episode_actions, episode_rewards = [], [], []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        action_probs = policy_net(torch.tensor(state).float())\n",
        "        action = torch.multinomial(action_probs, 1).item()\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        episode_states.append(state)\n",
        "        episode_actions.append(action)\n",
        "        episode_rewards.append(reward)\n",
        "\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # 方策の更新\n",
        "    update_policy(episode_states, episode_actions, episode_rewards)\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        print(f\"Episode {episode}: Total Reward: {total_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUF1yVSDBMvu",
        "outputId": "56f1149b-2e3d-43db-d681-831d742140a6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-5c94b1c07be4>:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  episode_states = torch.tensor(episode_states, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Total Reward: 13.0\n",
            "Episode 10: Total Reward: 16.0\n",
            "Episode 20: Total Reward: 13.0\n",
            "Episode 30: Total Reward: 22.0\n",
            "Episode 40: Total Reward: 13.0\n",
            "Episode 50: Total Reward: 15.0\n",
            "Episode 60: Total Reward: 13.0\n",
            "Episode 70: Total Reward: 18.0\n",
            "Episode 80: Total Reward: 19.0\n",
            "Episode 90: Total Reward: 19.0\n",
            "Episode 100: Total Reward: 13.0\n",
            "Episode 110: Total Reward: 13.0\n",
            "Episode 120: Total Reward: 14.0\n",
            "Episode 130: Total Reward: 15.0\n",
            "Episode 140: Total Reward: 12.0\n",
            "Episode 150: Total Reward: 14.0\n",
            "Episode 160: Total Reward: 15.0\n",
            "Episode 170: Total Reward: 12.0\n",
            "Episode 180: Total Reward: 26.0\n",
            "Episode 190: Total Reward: 14.0\n",
            "Episode 200: Total Reward: 20.0\n",
            "Episode 210: Total Reward: 9.0\n",
            "Episode 220: Total Reward: 25.0\n",
            "Episode 230: Total Reward: 11.0\n",
            "Episode 240: Total Reward: 14.0\n",
            "Episode 250: Total Reward: 14.0\n",
            "Episode 260: Total Reward: 14.0\n",
            "Episode 270: Total Reward: 16.0\n",
            "Episode 280: Total Reward: 10.0\n",
            "Episode 290: Total Reward: 12.0\n",
            "Episode 300: Total Reward: 12.0\n",
            "Episode 310: Total Reward: 11.0\n",
            "Episode 320: Total Reward: 16.0\n",
            "Episode 330: Total Reward: 11.0\n",
            "Episode 340: Total Reward: 13.0\n",
            "Episode 350: Total Reward: 10.0\n",
            "Episode 360: Total Reward: 11.0\n",
            "Episode 370: Total Reward: 19.0\n",
            "Episode 380: Total Reward: 12.0\n",
            "Episode 390: Total Reward: 23.0\n",
            "Episode 400: Total Reward: 9.0\n",
            "Episode 410: Total Reward: 11.0\n",
            "Episode 420: Total Reward: 14.0\n",
            "Episode 430: Total Reward: 17.0\n",
            "Episode 440: Total Reward: 17.0\n",
            "Episode 450: Total Reward: 17.0\n",
            "Episode 460: Total Reward: 13.0\n",
            "Episode 470: Total Reward: 21.0\n",
            "Episode 480: Total Reward: 11.0\n",
            "Episode 490: Total Reward: 19.0\n",
            "Episode 500: Total Reward: 12.0\n",
            "Episode 510: Total Reward: 16.0\n",
            "Episode 520: Total Reward: 21.0\n",
            "Episode 530: Total Reward: 15.0\n",
            "Episode 540: Total Reward: 10.0\n",
            "Episode 550: Total Reward: 16.0\n",
            "Episode 560: Total Reward: 15.0\n",
            "Episode 570: Total Reward: 16.0\n",
            "Episode 580: Total Reward: 11.0\n",
            "Episode 590: Total Reward: 12.0\n",
            "Episode 600: Total Reward: 15.0\n",
            "Episode 610: Total Reward: 18.0\n",
            "Episode 620: Total Reward: 15.0\n",
            "Episode 630: Total Reward: 15.0\n",
            "Episode 640: Total Reward: 15.0\n",
            "Episode 650: Total Reward: 13.0\n",
            "Episode 660: Total Reward: 14.0\n",
            "Episode 670: Total Reward: 19.0\n",
            "Episode 680: Total Reward: 12.0\n",
            "Episode 690: Total Reward: 12.0\n",
            "Episode 700: Total Reward: 14.0\n",
            "Episode 710: Total Reward: 13.0\n",
            "Episode 720: Total Reward: 11.0\n",
            "Episode 730: Total Reward: 13.0\n",
            "Episode 740: Total Reward: 10.0\n",
            "Episode 750: Total Reward: 15.0\n",
            "Episode 760: Total Reward: 14.0\n",
            "Episode 770: Total Reward: 13.0\n",
            "Episode 780: Total Reward: 21.0\n",
            "Episode 790: Total Reward: 17.0\n",
            "Episode 800: Total Reward: 11.0\n",
            "Episode 810: Total Reward: 14.0\n",
            "Episode 820: Total Reward: 13.0\n",
            "Episode 830: Total Reward: 26.0\n",
            "Episode 840: Total Reward: 11.0\n",
            "Episode 850: Total Reward: 16.0\n",
            "Episode 860: Total Reward: 16.0\n",
            "Episode 870: Total Reward: 16.0\n",
            "Episode 880: Total Reward: 16.0\n",
            "Episode 890: Total Reward: 23.0\n",
            "Episode 900: Total Reward: 12.0\n",
            "Episode 910: Total Reward: 15.0\n",
            "Episode 920: Total Reward: 14.0\n",
            "Episode 930: Total Reward: 13.0\n",
            "Episode 940: Total Reward: 10.0\n",
            "Episode 950: Total Reward: 10.0\n",
            "Episode 960: Total Reward: 18.0\n",
            "Episode 970: Total Reward: 18.0\n",
            "Episode 980: Total Reward: 15.0\n",
            "Episode 990: Total Reward: 12.0\n"
          ]
        }
      ]
    }
  ]
}