前の章では，我々はsuboptimality gapが0から1の区間となるようなサブガウシアンバンディットに対するミニマックスリグレットの下界を証明した．しかし，それらのバウンドは，方策のロバスト性のための良い指標として機能するが，時として保守的すぎる場合がある．この章では，特定のバンディットインスタンスにおける方策の最適なパフォーマ数を特定するために，**インスタンス依存の下界**について理解することを目的とする．

リグレットは多目的な批評家であるため，アルゴリズムの設計者はおそらくあるインスタンスでうまく動くようなアルゴリズムを設計しようとするだろう．何言ってるかわからないと思うので，極端な例として，方策がすべての$t$で行動$A_t = 1$をとるような場合を考える．方策は最初のアームが最適である場合はリグレットが0，それ以外は線形リグレットとなる．これは，対数的なリグレットをわずか数例で0にするかわりに，残りは線形リグレットになる．これはバンディットにおいては結構普通のことである．各インスタンスに難易度の指標を割り当てることができ，あるインスタンスでこの指標に対して過剰にうまく機能する方策は，他のインスタンスでは非常に大きな代償を払うことになる


![[Screenshot from 2024-04-11 13-31-01.png]]
座標$x$について，インスタンスはその難しさの指標の値の順に並べられている．座標$y$は，リグレットを示している．前の章では，我々は，どの方策も，ずっとミニマックスoptimalを下回ることはないとその証明とともにみてきた．この章における定理16.4は，もし，方策のリグレットがインスタンスoptimalのリミットの下に来た場合，それは斜線領域より上にリグレットがこなければならない．その例としてはover-specialisedな方策が挙げられる．有限時間に於いて，この状況はすこし煩雑である．しかしこれは正確な概念を得るために重要なステップである．

### 漸近境界(Asymptotic Bounds)

我々は，合理的な方策の意味を定義したい．もし，それが，漸近法だけに関心がある場合，保守的な定義で十分である．

#### 定義16.1
方策$\pi$がバンディット$\mathcal{E}$上において**consistent**(無矛盾？，一貫性のある)であると言われるためには，すべての$\nu \in \mathcal{E}$と，$p > 0$について以下を満たしている必要がある．
$$
\lim _{n \rightarrow \infty} \frac{R_n(\pi, \nu)}{n^p}=0 .
$$

$\mathcal{E}$ 上で一貫性のあるポリシーのクラスは $\Pi_{\text{cons}}(\mathcal{E})$と表記される．  
７章における定理7.1では，UCBは$\mathcal{E}_{\mathrm{SG}}^k(1)$上で一貫していることを示している．ただし，最初のアームがすべてのバンディットのインスタンスについて最適出ない場合を除いて，任意クラスの最初のアクションは一貫していない．  
この段落では，$\mathcal{E}$が確率バンディットの非構造的なクラスである場合，つまり$\mathcal{E}=\mathcal{M}_1 \times \cdots \times \mathcal{M}_k$であり，$\mathcal{M}_1, \ldots, \mathcal{M}_k$が分布の集合である場合を扱っている．本章の主要定理は，任意の非構造的確率バンディットクラスに適用可能な一般的な下限を示すものである．証明の後，特定のクラスへの応用を見ることになる．$\mathcal{M}$を有限の平均を持つ分布の集合とし，$\mu: \mathcal{M} \rightarrow \mathbb{R}$を$P \in \mathcal{M}$をその平均に写像する関数とする．$\mu^* \in \mathbb{R}$および$P \in \mathcal{M}$に対し$\mu(P)<\mu^*$であるとき，  
$$  
d_{\mathrm{inf}}\left(P, \mu^*, \mathcal{M}\right)=\inf _{P^{\prime} \in \mathcal{M}}\left\{\mathrm{D}\left(P, P^{\prime}\right): \mu\left(P^{\prime}\right)>\mu^*\right\}  
$$  
と定義する．  
  
ここで，$\mathcal{E}$が非構造的クラスであるとは，$\mathcal{E}$が独立した分布の直積で構成されていることを意味する．主要定理は，このようなクラスに対して，下限（つまり最小値）が存在することを示している．具体的には，$P^{\prime}$が$\mu^*$より大きい平均を持つ$\mathcal{M}$内の全ての分布に対して，$P$から$P^{\prime}$への距離の最小値を求めることになる．  
  
#### 定理16.2  
定理16.2では，$\mathcal{E}=\mathcal{M}_1 \times \cdots \times \mathcal{M}_k$とし，$\mathcal{E}$上の一貫した方針$\pi \in \Pi_{\text {cons }}(\mathcal{E})$について考察する．そこで，すべての$\nu=\left(P_i\right)_{i=1}^k \in \mathcal{E}$に対して，以下が成り立つ．  
$$  
\liminf _{n \rightarrow \infty} \frac{R_n}{\log (n)} \geq c^*(\nu, \mathcal{E})=\sum_{i: \Delta_i>0} \frac{\Delta_i}{d_{\inf }\left(P_i, \mu^*, \mathcal{M}_i\right)},  
$$  
ここで$\Delta_i$は$\nu$における第$i$アームの劣勢ギャップであり，$\mu^*$は最適アームの平均である．  
  
この定理は，$\mathcal{E}$上の一貫した方策を持つ場合，報酬の対数に対するレグレットの下限が存在することを示している．具体的には，無限大に近づくにつれて，平均リグレット$R_n$を$\log(n)$で割った値の下限は，$\nu$における各アームのsuboptimalギャップ$\Delta_i$とそのアームの分布$P_i$から最適な平均$\mu^*$を持つ分布までの最小距離$d_{\inf }\left(P_i, \mu^*, \mathcal{M}_i\right)$の比の和に等しくなる．  
  
ここで，$\Delta_i > 0$は，アーム$i$が最適なアームよりも低い平均報酬を持つことを示し，$d_{\inf }\left(P_i, \mu^*, \mathcal{M}_i\right)$はアーム$i$の分布と$\mu^*$より大きい平均を持つ$\mathcal{M}_i$内の分布との間の最小の距離を表す．この定理は，特定の環境$\mathcal{E}$で最適な方針を適用しても，避けられないリグレットが存在することを数学的に証明している．  
  
##### 証明
good noteにて．．．

表16.1は、一般的な選択肢の$\mathcal{M}$に対して$d_{\text {inf }}\left(P, \mu^*, \mathcal{M}\right)$の明確な公式を提供している。これらの量の計算はすべて直接的である（練習問題16.1）。$c^*(\nu, \mathcal{E})$の下限と定義は、ほとんどのクラス$\mathcal{E}$に対して、以下の式が成り立つ方針$\pi$が存在するという意味で、非常に基本的な量である。
$$
\lim _{n \rightarrow \infty} \frac{R_n(\pi, \nu)}{\log (n)}=c^*(\nu, \mathcal{E}) \quad \text { for all } \nu \in \mathcal{E} .
$$

これは、式(16.3)が成り立つ場合、方針をクラス$\mathcal{E}$に対して漸近的に最適と呼ぶことを正当化する。例えば、第8章のUCBと第10章のKL-UCBはそれぞれ、$\mathcal{E}_{\mathcal{N}}^k(1)$と$\mathcal{E}_{\mathcal{B}}^k$に対して漸近的に最適である。

この段落は、特定の確率バンディット問題のクラス$\mathcal{E}$に対して、時間が無限に近づくにつれて、選択された方針$\pi$によるレグレット$R_n(\pi, \nu)$が$\log(n)$に対して比例するという事実を強調している。この比例定数$c^*(\nu, \mathcal{E})$は、環境$\nu$およびクラス$\mathcal{E}$に基づいて計算され、方針$\pi$がクラス$\mathcal{E}$において漸近的に最適であるかどうかを判断する基準となる。この性質は、UCBやKL-UCBなどの特定の方針が、特定の問題クラスにおいて理論上最適であることを示している。

![[Pasted image 20240411143333.png]]
### 有限時間バウンド
一貫性の有限時間類似性をつくることで，有限時間のインスタンス依存バウンドを証明することができる．最初に，補題はBretagnolle–Huber不等式(14章)を連鎖させることで獲得することができるものをまとめる．これは，定理15.1をもちいて示す．

#### 補題16.3
$k$本腕の確率バンディット$\nu=\left(P_i\right)$と$\nu^{\prime}=\left(P_i^{\prime}\right)$が与えられており、これらは行動$i \in[k]$に対する報酬の分布のみが異なるとされる。$\nu$において$i$が劣勢であり、$\nu^{\prime}$において唯一の最適解であると仮定する。$\lambda=\mu_i\left(\nu^{\prime}\right)-\mu_i(\nu)$とする。すると、任意の方針$\pi$に対して、
$$
\mathbb{E}_{\nu \pi}\left[T_i(n)\right] \geq \frac{\log \left(\frac{\min \left\{\lambda-\Delta_i(\nu), \Delta_i(\nu)\right\}}{4}\right)+\log (n)-\log \left(R_n(\nu)+R_n\left(\nu^{\prime}\right)\right)}{\mathrm{D}\left(P_i, P_i^{\prime}\right)}
$$
が成り立つ。

ここで、$\mathbb{E}_{\nu \pi}\left[T_i(n)\right]$は方針$\pi$を適用した際に$n$回の試行で行動$i$が選ばれる回数の期待値を示す。$\Delta_i(\nu)$は$\nu$における行動$i$の劣勢ギャップ、$\lambda$は$\nu$と$\nu^{\prime}$における行動$i$の平均報酬の差、$\mathrm{D}\left(P_i, P_i^{\prime}\right)$は分布$P_i$と$P_i^{\prime}$間の距離を示す。この補題は、特定の行動が劣勢である環境からその行動が最適になる環境に変化する場合、その行動が選択される期待回数の下限を示している。この補題は，有限の$n$と任意の$\nu$に対して成立し，十分にリッチな環境クラスの有限時間インスタンス依存の下界に使える．これらの結果は漸近的な一貫性の概念がミニマックスリグレットがそこまで大きくならないような仮定によって書き換えられたようなガウシアンバンディットにおける有限時間インスタンス依存バウンドを提供する．この仮定だけで，どのようなインスタンスにおいても，minimax最適に限りなく近い方策はUCBよりはるかに良い方策にはなり得ないことを示すのに十分である．


#### 定理16.4  
定理16.4では，平均ベクトルが$\mu \in \mathbb{R}^k$で，劣勢ギャップが$\Delta \in[0, \infty)^k$の$k$本腕ガウスバンディット$\nu \in \mathcal{E}_{\mathcal{N}}^k$を考える．ここで$\mathcal{N}$は自然数の非空の部分集合である．  
$$  
\mathcal{E}(\nu)=\left\{\nu^{\prime} \in \mathcal{E}_{\mathcal{N}}^k: \mu_i\left(\nu^{\prime}\right) \in\left[\mu_i, \mu_i+2 \Delta_i\right], 1 \leq i \leq k\right\} .  
$$  
と定義される．  
  
ある定数$C>0$と$p \in(0,1)$が存在し，任意の$n \in \mathcal{N}$および$\nu^{\prime} \in \mathcal{E}(\nu)$に対して$R_n\left(\pi, \nu^{\prime}\right) \leq C n^p$を満たす方針$\pi$があるとする．このとき，任意の$\varepsilon \in(0,1]$に対して，  
$$  
\begin{aligned}  
& n \in \mathcal{N}, \\  
& \qquad R_n(\pi, \nu) \geq \frac{2}{(1+\varepsilon)^2} \sum_{i: \Delta_i>0}\left(\frac{(1-p) \log (n)+\log \left(\frac{\varepsilon \Delta_i}{8 C}\right)}{\Delta_i}\right)^{+}  
\end{aligned}  
$$  
が成り立つ．  
  
この定理は，$k$本腕ガウスバンディットにおける特定の方針$\pi$の下でのリグレットの下限を示している．ここで，$R_n(\pi, \nu)$は方針$\pi$を使用した際の$n$回の試行後の累積リグレットを示し，$(\cdot)^+$は括弧内の値が正の場合はその値を，そうでない場合は0を取る関数である．  
  
  
  
  
  
  
#### 参考文献(ここだけ翻訳しちゃった)  
漸近的な最適性に関して一貫性の仮定を最初に導入したのは，LaiとRobbinsによる1985年の基礎的な論文であり，後にBurnetasとKatehakisによって1996年に一般化された．上限に関しては，現在，単一パラメータ指数ファミリーに対して漸近的に最適な方針が存在する[Cappé et al., 2013]．最近まで，報酬分布の多パラメータクラスに対する漸近的最適性に関する結果は存在しなかった．平均と分散が未知のガウス分布[Cowan et al., 2018]および一様分布[Cowan and Katehakis, 2015]に対して最近進展があった．非パラメトリッククラスの報酬分布に関する漸近的に最適な戦略については，多くの未解決の問題が残っている．報酬分布が離散的で有限にサポートされている場合，漸近的に最適な方針はBurnetasとKatehakisによって1996年に提供されたが，正確な定数は解釈が難しい．有界サポートを持つクラスに対しては比較的完全な解決策が存在する[Honda and Takemura, 2010]．半有界の場合には，事情が複雑になっている[Honda and Takemura, 2015]．ある著者は，有界の尖度を持つクラスが非常に興味深いと考えているが，ここでは事柄が定数因子までしか理解されていない[Lattimore, 2017]．定理16.4の漸近的バリアントはSalomon et al. [2013]によって提案されている．有限時間のインスタンス依存の下限については，2つのアームに対してKulkarniとLugosi [2000]，一般的なケースに対してGarivier et al. [2019]とLattimore [2018]を含む複数の著者によって提案されている．先に述べたように，ETC方針や排除ベースのアルゴリズムは漸近的最適性を達成することはできない：Garivier et al. [2016b]によって示されているように，これらのアルゴリズム（どのように調整されても）は，標準的なガウスバンディット問題において最適な漸近的レグレットに比べて2倍の追加的な乗数ペナルティを負担しなければならない．  

---